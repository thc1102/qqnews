# “AI教母”李飞飞：要把大模型看成一个漂亮的计算器 不要把它当成威胁

# “AI教母”李飞飞：要把大模型看成一个漂亮的计算器 不要把它当成威胁

![](https://inews.gtimg.com/om_bt/O6fu8vlYKI1ggqBkPR91ybgRwBU6W7c0CNn690cZtg-
iYAA/1000)

腾讯科技讯 1月7日消息，据外媒报道，“AI教母”李飞飞博士最近接受外媒采访，谈到了如何对公众进行AI科普，如何更好地对AI进行监管等问题。

为AI革命奠定基础的研究者屈指可数，李飞飞就是其中之一。她15岁时从中国前往美国，一边求学一边打零工赚外快。如今，李飞飞博士是斯坦福大学计算机科学教授，也是该校
Human-Centered AI研究所的主管。

在外媒采访中，李飞飞表示AI不应该被当作鬼神来恐惧或者崇敬，而应该被视为一种工具，一种可以服务于人类利益的工具。李飞飞认为，必须始终重视人类的主观能动性。

**以下为外媒采访问题及李飞飞的回答：**

**问：科技产品越多，就会有越多的人想知道：如果没有它们的帮助，人类的心智会有怎样的产出呢？人类会带来什么价值呢？**

答：这就得从人的主观能动性开始说起了。人类是非常复杂的生物。我们不仅仅是由我们大脑某一侧或由我们在大数据上的计算方式来定义的。我们不是由我们的记忆负荷或任何我们自己的神经网络的算法来定义的。

**定义我们人类的，是我们的意志、我们的情感、我们的能动性、我们与自己和彼此的关系。** 作为AI领域的专家，最近我一直在强调，
**我们需要对自己有信心和自尊，因为我们和计算工具不一样。**

**问：为什么人类的这些能力很重要？**

答：有一个正确的“工具观”非常重要。科技产品非常强大，但我想强调的是，它们是工具。也许有人觉得这种想法有点书呆子气，但我在研究中看过一些调查材料，是关于美国人如何分配时间的。美国人把时间分配在工作、娱乐、休闲、家务上，其中包含的事项数以万计。

我并不想贬低科技产品，但 **与人类相比，科技产品能做的事情非常有限。我认为作为人类来讲，非常重要的一点是弄清楚我们与我们创造的工具之间的关系。**
这是人类文明一直面临的重大课题。有时候，我们能把这个关系处理好，有时则不然。我们需要理顺这种关系，用主观能动性来决定这种关系应该如何发展。

**问：有一些公司担心，把AI关在“防护栏”里会减缓创新的速度。应该怎样平衡创新的速度和安全性呢？**

答：这是一个价值万亿美元的问题。为这个问题找出解决办法非常重要。这会是一个持续不断的迭代过程。

我觉得这个问题的答案不会是简单直白的。坦率地说，如果任何人声称自己可以把解决办法用一两句话说清楚，我会觉得这个人没有直面现实。因为
**创新既要有速度，又要有安全性。**

创新将带来新的发现，带来就业机会，让我们有更高的生产力，更好的健康状况，更优质的教育资源，更适宜的环境。这些是可以预见的。

但与此同时，我们也需要“防护栏”来保护人的生命、人的尊严，特别是那些处境不佳的人。这涉及人类作为一个物种所关心的价值观。我作为一个技术专家，作为一个教育工作者，听到任何走极端的说法都会感到担心。

**问：怎样设计良好的“防护栏”呢？**

答：这是过去五年来我一直在努力的问题。这个问题促使我在斯坦福建立了Human-Centered AI 研究所。
**研究所名字的意思是，要把个人和社会的福祉放在设计、开发和部署AI的中心位置。**

设计和构建良好的防护栏是个非常复杂的事情。我认为需要一个框架，主张在技术和防护栏之间保持均衡，将伦理纳入到技术的设计之中，要有一套侧重于利益相关者的方法，来考虑技术对个人、社区和社会影响。

**问：公众对AI的理解，与专家相比，差距最大的地方在哪里？**

答：坦率地说，差距非常之大。

**AI技术太新了，公众不太了解它也很正常。** 公众当初了解电力这个东西也花了不少时间。我们必须给大家一些时间，让公众接受AI科普。

**目前公众被误导了，关注点被带偏了。倒也不是有人在恶意引导，而是因为缺乏沟通和科普。**

在舆论场上发声的差距也很大。有这么多优秀的研究人员、企业家、技术专家、教育家和政策制定者专注于创造更好的未来，比如把AI运用在医学或农业方面，但我们没有听到他们的声音。
**反而是一个想要“赢家通吃”的小团体把持了扩音器，这对公众有弊无利。**

**问：你想澄清的最大的误解是什么？**

答：举个例子， **大型语言模型可以帮助你做什么，这个问题就需要好好科普。**

有些人会直接从“大型语言模型出现了”跳跃到“人类的能动性全都消失了，没有人再需要学习英语”的结论上。有人会觉得，企业使用大型语言模型的时候，就是员工打开一个大型语言模型，然后离开房间，让它自己运行，产品就自动完成了——我认为这不太可能。

实际上， **大型语言模型就像一个漂亮的计算器，这个类比可能有点尬，但重点是，它们是工具，可以用来提高生产力。**

但我们也需要诚实地讨论一些问题，比如AI对工作和工资有什么影响？我们如何负责任地使用AI？但这些讨论并没有出现。如果你在大街上随机找一个普通的美国人，问他：你在哪里看到关于AI内容的？你从中了解了什么？你对AI的印象如何？他也许没啥可说的。

**问：好的AI教育是什么样？**

答：我不知道泰诺（Tylenol）感冒药是如何起作用的，但我相信它，也会服用它。所以教育也是分层次的。
**AI技术太新了，很多人都以为，除非自己懂数学，否则就弄不懂AI。但事情不是这样的。**

我们不需要了解生物化学，也能对泰诺有一个常识性的了解。公共教育会告诉你，什么药物是用来治疗什么症状的，药物监管部门做了什么，以及患者怎样与药剂师和医生合作，以负责任的方式服用泰诺。

现有的公共教育提高了大众的主观能动性，让他们理解了一些常识。然后，如果你真的对生物化学感兴趣，你自己会想去研究泰诺的分子效应。

目前的AI教育缺少就这一块。虽然技术方面有很多公开的资料，但并不是很容易理解。从某种程度上说，这就是我写《我看到的世界》这本书的原因——我想用一种通俗易懂的方式来讨论AI。我们需要对公众进行科普，与他们沟通，来了解大家如何从其他角度来看待AI，比如经济学或立法角度。

**问：如果没有正确的教育，会有什么后果？**

答：我最担心的是大家失去主观能动性。很多人都很害怕，担心人类最终会沦落到被机器统治。这也太夸张了吧。但如果公共教育不足，这可能会成为一个自我实现的预言。

我们都有主观能动性。政策制定真的很重要。它可以全面禁止整个AI，也可以让情况变得失控，而失控会比禁止更糟糕。为了人类的尊严，就必须要有合适的防护栏。这样我们才能用AI来提供福祉。

**问：每次我在派对上跟人说，我（外媒记者）是靠写作谋生的，他们都会说ChatGPT将抢走我的工作。我该怎么回应？**

答：你低估了自己的主观能动性！

不妨做个实验，你找个题目，让ChatGPT写出来，然后你用红色标出你不得不重写的部分，拿出来给大家展示一下：看吧，有95%都要修改。

我的意思是，也许有一些事情，你放手给ChatGPT做，是因为它真的做得不错，但仔细看看你就会发现，你自己还有很大的发挥空间。
**你可以用AI来提高工作效率，激发你的主观能动性，不要把它当成威胁。** （编译/云开）

